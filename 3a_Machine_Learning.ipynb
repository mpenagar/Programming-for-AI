{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d6a65d-3391-430d-9113-5ffbe6972fef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "1. Fundamentals of Machine Learning\n",
    "1. Learning Paradigms and Machine Learning Tasks\n",
    "1. Task vs Dataset - Learning the wrong task\n",
    "1. Performance metrics in Machine Learning\n",
    "1. Loss functions in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152331f-461a-4bda-817d-7e33159ddedd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1. Fundamentals of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee68866-a231-4147-95da-f6bc5617cd4d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* **Learn from Data**. Computers can learn from data without being explicitly programmed for every specific task (<u>rule-based</u> &rarr; <u>data-driven</u>). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f6a22-bf4b-43b2-8a61-8f680b4d2361",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* **Identify Patterns**. Algorithms can automatically find patterns, relationships, and insights within data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f888b333-5881-44d9-ae4f-00e9354d7381",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* **Generalization**. Ability to make predictions on new, unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c826b6-50a4-4ca6-993f-33d56e5226a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* **Algorithms and Models**. Algorithms (sets of instructions) are used to build models (representations of the learned patterns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168170a3-91ed-4e2d-89cd-b4f387552d03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2. Learning Paradigms and Machine Learning Tasks\n",
    "\n",
    "Machine learning tasks can be categorized according to the learning paradigm and the specific goal they pursue:\n",
    "* Supervised Learning\n",
    "* Unsupervised Learning\n",
    "* Self-Supervised Learning\n",
    "* Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b50337-9f3f-4388-a6ac-2e319bed2a42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Supervised Learning\n",
    "\n",
    "* Each input data is associated with a known output or target variable.\n",
    "* **Goal**: Learn a *mapping* from input to output data.\n",
    "\n",
    "Some common tasks: **Classification** & **Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c59ba8-5c08-42bc-82e0-927c83111283",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Classification Task\n",
    "* **Goal**: Assign data points to <u>predefined</u> categories or classes.\n",
    "* The output variable is categorical.\n",
    "    * **Binary Classification**: Predicting one of two classes.\n",
    "    * **Multi-class Classification**: Predicting one of more than two classes.\n",
    "* Examples:\n",
    "    * <u>Image Classification</u>: Identifying objects in an image (e.g., cat, dog, car). Â  \n",
    "    * <u>Spam Detection</u>: Classifying emails as spam or not spam. Â  \n",
    "    * <u>Medical Diagnosis</u>: Determining if a patient has a certain disease based on symptoms and test results. Â  \n",
    "    * <u>Fraud Detection</u>: Identifying fraudulent transactions based on user behavior and transaction details. Â  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f04bb6-373a-4af1-8de0-4cbe5cf43f1a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "A model that fits a type of task can be used <u>regardless of what the inputs and outputs represent</u>. \n",
    "\n",
    "* If a model fits a binary text classification task, it can be used for:\n",
    "    * Spam Detection: Spam &harr; Not Spam\n",
    "    * Topic Classification: Relevant &harr; Irrelevant\n",
    "    * Fake News Detection: Fake &harr; Real\n",
    "    * Bot Detection in Social Media: Bot &harr; Human\n",
    "    * Political Orientation Classification: right &harr; left political ideology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a62b5d6-2bee-4ff0-b33c-51c203a10486",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Regression Task\n",
    "* **Goal**: Predict a continuous numerical value.\n",
    "* The output variable is continuous.\n",
    "    * **Linear Regression**: Linear relationship with the input features.\n",
    "    * **Polynomial Regression**: Polynomial relationship with the input features.\n",
    "    * ... Â  \n",
    "    * **Time Series Forecasting**: Predicting future values based on past values\n",
    "* Examples:\n",
    "    * <u>Feature-based Estimations</u>\n",
    "        * Predict a house selling price given features like the size of the house, number of bedrooms, location, and age.\n",
    "        * Insurance risk estimation based on customer demographics, driving history, and other risk factors\n",
    "    * <u>Feature & Time based Estimations</u> (Time Series Forecasting)\n",
    "        * Stock Market Prices estimation based on historical stock data, economic indicators, and company performance.\n",
    "        * Energy demand prediction based on weather patterns, time of day, and historical demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f814a-8ba9-4747-b45d-154046e67eb8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Categorical vs discrete output variables\n",
    "\n",
    "Output variable can be discrete, but not categorical:\n",
    "* <u>Sentiment Analysis</u>, e.g. classifying text (e.g., reviews, tweets) as positive, negative, or neutral.\n",
    "* <u>Rating Prediction</u>, e.g. predicting a song or movie rating on a scale of 1 to 5 stars\n",
    "    \n",
    "Different approaches:\n",
    "* Use a regression model and discretize the estimation\n",
    "* Use a classification model (with <u>loss of ordinal information or ignoring numerical relationships</u>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5423a2f-ec17-44c1-bf12-6aa5166f6b5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "* The algorithm learns from unlabeled data, without any explicit output or target variable.\n",
    "* **Goal**: Discover hidden patterns or structure in the (input) data\n",
    "\n",
    "Some common tasks: Clustering & Dimensionality Reduction (PCA, t-SNE, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3774af-90ea-4bde-a39f-6b0f937c9dab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Self-Supervised Learning\n",
    "\n",
    "* The algorithm learns useful representations of the data by creating \"artificial\" labels from the unlabeled data itself.\n",
    "    * E.g. a system that creates a lower-dimensional representation of the (corrupted) input data and tries to reconstruct the original (clean) input data.\n",
    "* **Goal**: Learn useful representations from the (input) data.\n",
    "\n",
    "Some common tasks: Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24175700-1dd2-4d6e-9cd6-878c26f8f101",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Word embeddings\n",
    "\n",
    "* Low-dimensional (50-300) numerical representation of words\n",
    "* Capture the semantic meaning and relationships between words\n",
    "    * Words with similar meanings are close in the embedding space\n",
    "* Mathematical operations reflect semantic relationships\n",
    "    * `vector(\"king\")` - `vector(\"man\")` + `vector(\"woman\")` â‰ˆ `vector(\"queen\")`\n",
    "* Language dependent, already available for many languages "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c98f875-b335-4060-b8a9-a91af6e15098",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "* The algorithm (*agent*) learns through trial and error by interacting with the environment and observing the consequences of its actions (rewards/penalties).\n",
    "    * Like learning through experience\n",
    "* **Goal**: Learn an optimal policy to maximize cumulative reward.\n",
    "\n",
    "Some common tasks: Game Playing, Autonomous Driving, Chats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a629c2-6c30-4d3d-8895-5a11fb295226",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3. Task vs Dataset - Learning the wrong task\n",
    "\n",
    "* The model solves the problem present in the dataset\n",
    "    * Input data &rarr; Output data\n",
    "    * The database may not represent the task it is intended to.\n",
    "    * Dataset bias &rarr; learn **spurious correlations**\n",
    "* Random sampling for unbiased datasets\n",
    "    * No selection bias\n",
    "    * Better reflects the true underlying distribution of the population you want your model to generalize to.\n",
    "    * Machine learning statistical methods assume that the data is a random sample.\n",
    "* In most cases (almost all), datasets are not created from random samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d101fc2-2cc9-48a6-b5e2-0a7056fee38c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Some examples of dataset bias:\n",
    "* Alzheimer detection from voice. Dataset: Recordings of people who have developed the disease and healthy people &rarr; voice disorder, age, channel...\n",
    "* Language identification in phone calls.  Dataset: phone calls &rarr; age, gender, channel...\n",
    "* Fake news detection. Dataset: Fake news from Twitter vs news agencies (AP, Reuters...) &rarr; source format\n",
    "* AI-generated text detection: Dataset some text created by humans and AIs &rarr; spelling error, rich vocabulary... (maybe not a spurious correlation? ðŸ˜…)\n",
    "\n",
    "<center><b>Always question what task you are solving</b></center>\n",
    "\n",
    "Even more if your model results are surprisingly good..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f63f22a-4ab6-4e30-8486-7dcce7c213d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 4. Performance metrics in Machine Learning\n",
    "\n",
    "* Get an estimation of the performance of the model.\n",
    "* Depending on the concrete task, there are different metrics\n",
    "    * Binary Classification\n",
    "    * Multiclass Classification\n",
    "    * Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f023be5-cdd2-4212-9e80-2d84e102f1f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Binary Classification - Confusion Matrix\n",
    "\n",
    "* Visualizes the performance by comparing the predicted labels to the actual labels:\n",
    "\n",
    "|                     | **Predicted Positive** | **Predicted Negative** |\n",
    "|---------------------|------------------------|------------------------|\n",
    "| **Actual Positive** | True Positive (**TP**)     | False Negative (**FN**)    |\n",
    "| **Actual Negative** | False Positive (**FP**)    | True Negative (**TN**)     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7186799d-ec66-4ed5-b40d-62910582b05b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Binary Classification - Aggregate Metrics\n",
    "\n",
    "* **Accuracy**. The most intuitive metric:\n",
    "$$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} \\in [0,1]$$\n",
    "* **Precision**. The ratio of TP instances to all the instances the model predicted as positive. Useful in false-positive high-cost scenarios.\n",
    "$$Precision = \\frac{TP}{TP+FP} \\in [0,1]$$\n",
    "* **Recall (Sensitivity, True Positive Rate)**. The ratio of TP instances to all the actual positive instances. Useful in false-negative high-cost scenarios.\n",
    "$$Recall = \\frac{TP}{TP+FN} \\in [0,1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daebb4b-25b3-42e7-a375-be55d174e3b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Binary Classification - Aggregate Metrics\n",
    "\n",
    "* **F1-Score**. Harmonic mean of precision and recall (penalizes extreme values).\n",
    "$$F1-Score = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision+Recall} = \\frac{2 \\cdot TP}{2 \\cdot TP + FN + FN} \\in [0,1]$$\n",
    "* **Specificity (Sensitivity, True Negative Rate)**. The ratio of TN instances to all the actual negative instances.\n",
    "$$Specificity = \\frac{TN}{TN+FP} \\in [0,1]$$\n",
    "* **False Positive Rate**. The ratio of FP instances to all the actual negative instances.\n",
    "$$FPR = \\frac{FP}{TN+FP} = 1 - Specificity \\in [0,1]$$\n",
    "* **False Negative Rate**. The ratio of FN instances to all the actual positive instances.\n",
    "$$FNR = \\frac{FN}{TP+FN} = 1 - Recall \\in [0,1]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9ef2e-5cbc-42a4-b0b7-b01d47ea9c88",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Binary Classification - ROC curve\n",
    "\n",
    "**ROC Curve**: Receiver Operating Characteristic Curve.\n",
    "\n",
    "* Given input data $x$, binary classifiers tipically use a threshold over the posterior probability $P(y=1|x)$ to decide positive/negative output.\n",
    "    * If $P(y=1âˆ£x) \\ge threshold$, predict the $y=1$ positive class.\n",
    "    * If $P(y=1âˆ£x) < threshold$, predict the $y=0$ negative class.\n",
    "    * Calibrated system and balanced error costs (FP vs FN) &rarr; $threshold = 0.5$\n",
    "    * $threshold \\approx$ **operation point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b3ce9-f726-49af-96f6-b8d297fa6082",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* ROC curve plots the True Positive Rate (TPR) vs the False Positive Rate (FPR) over $threshold \\in [0,1]$\n",
    "    * $threshold = 1$ &rarr; all negatives &rarr; $TPR=1$ and $FPR=1$\n",
    "    * $threshold = 0$ &rarr; all positives &rarr; $TPR=0$ and $FPR=0$\n",
    "\n",
    "<center><img src=\"img/roc.webp\" alt=\"Receiver Operating Characteristic Curve\" style=\"width: 50%;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77455e91-1c4b-4bd5-be0f-ee487a5effd0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Binary Classification - AUC\n",
    "\n",
    "**AUC**:  Area Under the (ROC) Curve\n",
    "\n",
    "* Aggregate metric based on ROC\n",
    "    * Perfect Classifier: $AUC = 1$\n",
    "    * Random Classifier: $AUC = 0.5$\n",
    "    * Better than Random Classifier: $AUC > 0.5$\n",
    "    * Worse than Random Classifier: $AUC < 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe96fd8b-79ab-4751-8b76-798500b63e00",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Multiclass Classification - Confusion Matrix\n",
    "\n",
    "* $n$-class Classification task &rarr; $n \\times n$ confusion matrix\n",
    "\n",
    "| Actual \\ Predicted | Class 1 | ... | Class n |\n",
    "| :------------------ | :-: | :-: | :-: |\n",
    "| **Class 1** | $C_{11}$  | $\\dots$   | $C_{1n}$   |\n",
    "| $\\vdots$ | $\\vdots$  | $\\ddots$  | $\\vdots$   |\n",
    "| **Class n** | $C_{n1}$  | $\\dots$   | $C_{nn}$  |\n",
    "\n",
    "* **Accuracy**. The ratio of correctly classified samples to the total number of samples\n",
    "$$Accuracy = \\frac{trace(C)}{sum(C)} = \\frac{\\sum_{i=1}^{n}{C_{ii}}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n}{C_{ij}}} \\in [0,1]$$\n",
    "\n",
    "* **Precision, Recall, and F1-Score** (Per Class and Averaged) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f11c3-c10d-4f4a-9125-f6a6b860e37a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Regression - Performance Metrics \n",
    "\n",
    "* **GOAL**: Quantify the difference between the predicted/actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fdd87b-52da-4a37-bf02-3159bf434fdd",
   "metadata": {},
   "source": [
    "* **Mean Absolute Error (MAE)**: $\\frac{1}{n} \\sum_{i=1}^{n}{|y_i - \\hat{y}_i|}$\n",
    "    * Average prediction error "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4b77a-098d-46c7-be7b-87aa0816fbe2",
   "metadata": {},
   "source": [
    "* **Mean Squared Error (MSE)**: $\\frac{1}{n} \\sum_{i=1}^{n}{(y_i - \\hat{y}_i)^2}$\n",
    "    * Sensitive to outliers (penalizes larger errors)\n",
    "    * Squared units relative to target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7522e-c38d-4055-ab7b-32bf664a9427",
   "metadata": {},
   "source": [
    "* **Root Mean Squared Error (RMSE)**: $\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{(y_i - \\hat{y}_i)^2}}$\n",
    "    * Sensitive to outliers (penalizes larger errors)\n",
    "    * Same units as target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44f747-be9c-462a-93a5-73e073bcc510",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 5. Loss Functions in Machine Learning\n",
    "\n",
    "<center><b>Loss</b> Function = <b>Cost</b> Function = <b>Objective</b> Function</center>\n",
    "\n",
    "* Quantifies the error/discrepancy between the predicted and the actual (true) output.\n",
    "* The purpose of training is to **minimize the loss**.\n",
    "* Intuitively, &nbsp; $performance \\; metric \\;\\to\\; loss \\; function$\n",
    "* Most optimization algorithms (gradient-based methods) impose some restrictions on loss functions:\n",
    "    * Differentiability (at least sub-differentiability)\n",
    "    * Convexity (preferred)\n",
    "    * Lack of large flat regions or plateaus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b33e8-02d4-4855-8ac0-0c9989639b79",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **Accuracy**, **Precision**, **Recall**, and **F1-score**\n",
    "    * Step functions (Piecewise Constant). Based on discrete predictions (TP, FP, FN, TN)) \n",
    "    * Not differentiable and where differentiable, flat.\n",
    "    * &rarr; **Cannot be used**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71732619-0313-4d01-8c26-2a76eb18b4e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* **Mean Absolute Error (MAE)**\n",
    "    * Differentiable (except at zero error)\n",
    "    * Constant gradient ($\\pm 1$) &rarr; Prone to oscillations around the minimum.\n",
    "    * **Can be used** for gradient-based optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814fb600-4617-47a9-99d4-30d3837ecae7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* **Mean Squared Error (MSE)** and **Root Mean Squared Error (RMSE)**\n",
    "    * Differentiable\n",
    "    * **Well-suited** for gradient-based optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684d2e4-26f7-4118-8aca-017a8dd80f8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Classification tasks - Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fd754-ece9-48b5-9a7f-bb6d7b5f21ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **Cross-Entropy**\n",
    "    * The average number of bits needed to identify/represent an output data $y$ given an input data $x$ and the knowledge provided by the model.\n",
    "    * The average amount of **knowledge** provided by the model.\n",
    "$$L = \\frac{1}{n} \\sum_{i=1}^{n} - \\log(p(y=y_i | \\mathbf{x}_i))$$\n",
    "    * Measured in nats (base $e$) or bits (base $2$)\n",
    "    * Perfect Classifier: $L = 0$\n",
    "    * Non-Informative Classifier: $L = \\log(N)$ &nbsp; ($N$: number of classes)\n",
    "    * Better than Non-Informative Classifier: $L < \\log(N)$\n",
    "    * Worse than Non-Informative Classifier: $L > \\log(N)$ <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad6df11-9447-4f56-b1f4-61ea5bdee4be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **Hinge Loss** <br><br>\n",
    "* **Squared Hinge Loss** <br><br>\n",
    "* **Kullback-Leibler Divergence** <br><br>\n",
    "* **Focal Loss** <br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
